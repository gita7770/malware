# Adversarial Attack Code with Visualization
import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
import numpy as np
import tensorflow as tf
import matplotlib
matplotlib.use('Agg')  # Use Agg backend which doesn't require a display
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

""" Hyperparameters """
hp = {}
hp["image_size"] = 200
hp["num_channels"] = 3
hp["patch_size"] = 25
hp["num_patches"] = (hp["image_size"]**2) // (hp["patch_size"]**2)
hp["flat_patches_shape"] = (hp["num_patches"], hp["patch_size"]*hp["patch_size"]*hp["num_channels"])

hp["batch_size"] = 16
hp["lr"] = 1e-4
hp["num_epochs"] = 500
hp["num_classes"] = 5
hp["class_names"] = ["adware", "addisplay", "benign", "trojan", "riskware"]
hp["num_layers"] = 12
hp["hidden_dim"] = 768
hp["mlp_dim"] = 3072
hp["num_heads"] = 12
hp["dropout_rate"] = 0.1

def fgsm_attack(model, images, labels, epsilon):
    images = tf.cast(images, tf.float32)
    labels = tf.cast(labels, tf.float32)
    with tf.GradientTape() as tape:
        tape.watch(images)
        predictions = model(images)
        loss = tf.keras.losses.categorical_crossentropy(labels, predictions)
    gradient = tape.gradient(loss, images)
    signed_grad = tf.sign(gradient)
    adversarial_images = images + epsilon * signed_grad
    adversarial_images = tf.clip_by_value(adversarial_images, 0, 1)
    return adversarial_images

def pgd_attack(model, images, labels, epsilon, alpha, num_iter):
    adv_images = images
    for i in range(num_iter):
        adv_images = fgsm_attack(model, adv_images, labels, alpha)
        adv_images = tf.clip_by_value(images - epsilon, 0, 1) + tf.clip_by_value(adv_images - (images - epsilon), 0, 2*epsilon) - epsilon
    return adv_images

def plot_images(original, perturbed, adversarial, index, save_path):
    fig, axs = plt.subplots(1, 3, figsize=(15, 5))
    axs[0].imshow(original[index])
    axs[0].set_title('Original Image')
    axs[1].imshow(perturbed[index])
    axs[1].set_title('Perturbed Image')
    axs[2].imshow(adversarial[index])
    axs[2].set_title('Adversarial Image')
    
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    file_name = os.path.join(save_path, f'image_{index}.png')
    plt.savefig(file_name)
    plt.close()

def plot_confusion_matrix(y_true, y_pred, labels):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot()
    plt.show()

if __name__ == "__main__":
    """ Load model """
    model_path = os.path.join("files", "model.h5")
    model = tf.keras.models.load_model(model_path, compile=False)

    """ Load data """
    dataset_path = "/content/train"
    _, _, test_x = load_data(dataset_path)
    test_ds = tf_dataset(test_x, batch=hp["batch_size"])

    """ Evaluate model """
    y_true, y_pred = [], []
    for images, labels in test_ds:
        y_true.extend(np.argmax(labels.numpy(), axis=-1))
        preds = model.predict(images)
        y_pred.extend(np.argmax(preds, axis=-1))

    print("Classification Report:")
    print(classification_report(y_true, y_pred, target_names=hp["class_names"]))

    """ FGSM Attack """
    epsilon = 0.01
    y_true_adv, y_pred_adv = [], []
    fgsm_save_path = "fgsm_images"
    for images, labels in test_ds:
        adv_images = fgsm_attack(model, images, labels, epsilon)
        y_true_adv.extend(np.argmax(labels.numpy(), axis=-1))
        preds_adv = model.predict(adv_images)
        y_pred_adv.extend(np.argmax(preds_adv, axis=-1))

        # Save original, perturbed, and adversarial images
        for i in range(len(images)):
            plot_images(images.numpy(), adv_images.numpy(), adv_images.numpy(), i, fgsm_save_path)
            if i == 2:  # Save for 3 images
                break

    print("Adversarial Classification Report:")
    print(classification_report(y_true_adv, y_pred_adv, target_names=hp["class_names"]))

    """ PGD Attack """
    alpha = 0.005
    num_iter = 10
    y_true_pgd, y_pred_pgd = [], []
    pgd_save_path = "pgd_images"
    for images, labels in test_ds:
        adv_images_pgd = pgd_attack(model, images, labels, epsilon, alpha, num_iter)
        y_true_pgd.extend(np.argmax(labels.numpy(), axis=-1))
        preds_adv_pgd = model.predict(adv_images_pgd)
        y_pred_pgd.extend(np.argmax(preds_adv_pgd, axis[-1]))

        # Save original, perturbed, and adversarial images
        for i in range(len(images)):
            plot_images(images.numpy(), adv_images.numpy(), adv_images_pgd.numpy(), i, pgd_save_path)
            if i == 2:  # Save for 3 images
                break

    print("PGD Adversarial Classification Report:")
    print(classification_report(y_true_pgd, y_pred_pgd, target_names=hp["class_names"]))

    """ Plot Confusion Matrix """
    plot_confusion_matrix(y_true, y_pred, hp["class_names"])
    plot_confusion_matrix(y_true_adv, y_pred_adv, hp["class_names"])
    plot_confusion_matrix(y_true_pgd, y_pred_pgd, hp["class_names"])
