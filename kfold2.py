import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
import numpy as np
import cv2
from glob import glob
from sklearn.utils import shuffle
from sklearn.model_selection import KFold
from patchify import patchify
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping
from vit import ViT

""" Hyperparameters """
hp = {
    "image_size": 200,
    "num_channels": 3,
    "patch_size": 25,
    "num_patches": (200**2) // (25**2),
    "flat_patches_shape": (64, 25*25*3),
    "batch_size": 32,
    "lr": 1e-4,
    "num_epochs": 1,
    "num_classes": 5,
    "class_names": ["adware", "addisplay", "benign", "trojan", "riskware"],
    "num_layers": 12,
    "hidden_dim": 768,
    "mlp_dim": 3072,
    "num_heads": 12,
    "dropout_rate": 0.1
}

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def load_data(path):
    images = shuffle(glob(os.path.join(path, "*", "*.png")))
    return images

def process_image_label(path):
    """ Reading images """
    path = path.decode()
    image = cv2.imread(path, cv2.IMREAD_COLOR)
    image = cv2.resize(image, (hp["image_size"], hp["image_size"]))
    image = image / 255.0

    """ Preprocessing to patches """
    patch_shape = (hp["patch_size"], hp["patch_size"], hp["num_channels"])
    patches = patchify(image, patch_shape, hp["patch_size"])
    patches = np.reshape(patches, hp["flat_patches_shape"])
    patches = patches.astype(np.float32)

    """ Label """
    class_name = path.split("/")[-2]
    class_idx = hp["class_names"].index(class_name)
    class_idx = np.array(class_idx, dtype=np.int32)

    return patches, class_idx

def parse(path):
    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])
    labels = tf.one_hot(labels, hp["num_classes"])

    patches.set_shape(hp["flat_patches_shape"])
    labels.set_shape(hp["num_classes"])

    return patches, labels

def tf_dataset(images, batch=32):
    ds = tf.data.Dataset.from_tensor_slices((images))
    ds = ds.map(parse).batch(batch).prefetch(tf.data.experimental.AUTOTUNE)
    return ds

if __name__ == "__main__":
    """ Seeding """
    np.random.seed(42)
    tf.random.set_seed(42)

    """ Directory for storing files """
    create_dir("files")

    """ Paths """
    train_path = "/content/malware/Gitufinaldataset/3okmalika"
    valid_path = "/content/malware/Gitufinaldataset/test"
    test_path = "/content/malware/Gitufinaldataset/test"
    model_path_template = os.path.join("files", "model_fold_{fold_no}.keras")
    csv_path = os.path.join("files", "log_fold_{fold_no}.csv")

    """ Load data """
    train_images = load_data(train_path)
    valid_images = load_data(valid_path)
    test_images = load_data(test_path)

    print(f"Total Train images: {len(train_images)}")
    print(f"Total Validation images: {len(valid_images)}")
    print(f"Total Test images: {len(test_images)}")

    """ K-Fold Cross-Validation """
    kfold = KFold(n_splits=5, shuffle=True, random_state=42)

    fold_no = 1
    for train_idx, val_idx in kfold.split(train_images):
        fold_train_images = np.array(train_images)[train_idx]
        fold_valid_images = np.array(train_images)[val_idx]

        print(f"Fold {fold_no} - Train: {len(fold_train_images)} - Valid: {len(fold_valid_images)}")

        train_ds = tf_dataset(fold_train_images, batch=hp["batch_size"])
        valid_ds = tf_dataset(valid_images, batch=hp["batch_size"])

        """ Model """
        model = ViT(hp)
        model.compile(
            loss="categorical_crossentropy",
            optimizer=tf.keras.optimizers.Adam(hp["lr"], clipvalue=1.0),
            metrics=["acc"]
        )

        callbacks = [
            ModelCheckpoint(model_path_template.format(fold_no=fold_no), monitor='val_loss', verbose=1, save_best_only=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),
            CSVLogger(csv_path.format(fold_no=fold_no)),
            EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),
        ]

        model.fit(
            train_ds,
            epochs=hp["num_epochs"],
            validation_data=valid_ds,
            callbacks=callbacks
        )

        fold_no += 1

    """ Evaluate on test data """
    test_ds = tf_dataset(test_images, batch=hp["batch_size"])
    test_loss, test_acc = model.evaluate(test_ds)
    print(f"Test Loss: {test_loss}, Test Accuracy: {test_acc}")
