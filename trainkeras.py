import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import numpy as np
import tensorflow as tf
from glob import glob
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from patchify import patchify
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping
from vit import ViT

""" Hyperparameters """
hp = {}
hp["image_size"] = 200
hp["num_channels"] = 3
hp["patch_size"] = 25
hp["num_patches"] = (hp["image_size"]**2) // (hp["patch_size"]**2)
hp["flat_patches_shape"] = (hp["num_patches"], hp["patch_size"]*hp["patch_size"]*hp["num_channels"])

hp["batch_size"] = 32
hp["lr"] = 1e-4
hp["num_epochs"] = 100
hp["num_classes"] = 5
hp["class_names"] =  ["adware", "addisplay", "benign", "trojan", "riskware"]

hp["num_layers"] = 12
hp["hidden_dim"] = 768
hp["mlp_dim"] = 3072
hp["num_heads"] = 12
hp["dropout_rate"] = 0.1

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def load_data(path, split=0.1):
    images = shuffle(glob(os.path.join(path, "*", "*.png")))

    split_size = int(len(images) * split)
    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)
    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)

    return train_x, valid_x, test_x

def process_image_label(path):
  path = path.decode()
    image = cv2.imread(path, cv2.IMREAD_COLOR)
    
    # Check if image is read correctly
    if image is None:
        print(f"Error reading image at path: {path}")
        return np.zeros(hp["flat_patches_shape"], dtype=np.float32), -1
    
    image = cv2.resize(image, (hp["image_size"], hp["image_size"]))
    image = image / 25

    """ Preprocessing to patches """
    patches = tf.image.extract_patches(
        images=tf.expand_dims(image, 0),
        sizes=[1, hp["patch_size"], hp["patch_size"], 1],
        strides=[1, hp["patch_size"], hp["patch_size"], 1],
        rates=[1, 1, 1, 1],
        padding='VALID'
    )
    patches = tf.reshape(patches, hp["flat_patches_shape"])

    """ Label """
    class_name = tf.strings.split(path, os.sep)[-2]
    class_idx = tf.where(tf.equal(hp["class_names"], class_name))[0, 0]
    class_idx = tf.cast(class_idx, tf.int32)
    label = tf.one_hot(class_idx, hp["num_classes"])

    return patches, label

def parse(path):
    patches, label = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])
    patches.set_shape(hp["flat_patches_shape"])
    label.set_shape([hp["num_classes"]])
    return patches, label

def tf_dataset(images, batch=32):
    ds = tf.data.Dataset.from_tensor_slices(images)
    ds = ds.map(parse, num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.batch(batch)
    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)
    return ds

if __name__ == "__main__":
    """ Seeding """
    np.random.seed(42)
    tf.random.set_seed(42)

    """ Directory for storing files """
    create_dir("files")

    """ Paths """
    dataset_path = "/content/50Kdataset"
    model_path = os.path.join("files", "model.keras")
    csv_path = os.path.join("files", "log.csv")

    """ Dataset """
    train_x, valid_x, test_x = load_data(dataset_path)
    print(f"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}")

    train_ds = tf_dataset(train_x, batch=hp["batch_size"])
    valid_ds = tf_dataset(valid_x, batch=hp["batch_size"])

    """ Model """
    model = ViT(hp)
    model.compile(
        loss="categorical_crossentropy",
        optimizer=tf.keras.optimizers.Adam(hp["lr"], clipvalue=1.0),
        metrics=["acc"]
    )

    callbacks = [
        ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),
        CSVLogger(csv_path),
        EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=False),
    ]

    model.fit(
        train_ds,
        epochs=hp["num_epochs"],
        validation_data=valid_ds,
        callbacks=callbacks
    )
