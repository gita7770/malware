import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import numpy as np
import cv2
from glob import glob
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from patchify import patchify
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping
from vit import ViT

""" Enable Mixed Precision for Faster Training """
tf.keras.mixed_precision.set_global_policy("mixed_float16")

""" Hyperparameters """
hp = {
    "image_size": 200,
    "num_channels": 3,
    "patch_size": 25,
    "num_patches": (200**2) // (25**2),
    "flat_patches_shape": ((200**2) // (25**2), 25 * 25 * 3),
    "batch_size": 16,
    "lr": 1e-4,
    "num_epochs": 500,
    "num_classes": 5,
    "class_names": ["adware", "addisplay", "benign", "trojan", "riskware"],
    "num_layers": 8,
    "hidden_dim": 512,
    "mlp_dim": 2048,
    "num_heads": 8,
    "dropout_rate": 0.1
}

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def load_data(path, val_split=0.2, test_split=0.1):
    images = shuffle(glob(os.path.join(path, "*", "*.png")))

    val_size = int(len(images) * val_split)
    test_size = int(len(images) * test_split)

    train_x, temp_x = train_test_split(images, test_size=(val_size + test_size), random_state=42)
    valid_x, test_x = train_test_split(temp_x, test_size=test_size, random_state=42)

    return train_x, valid_x, test_x

def process_image_label(path):
    path = path.decode()
    image = cv2.imread(path, cv2.IMREAD_COLOR)
    image = cv2.resize(image, (hp["image_size"], hp["image_size"]))
    image = image / 255.0

    patch_shape = (hp["patch_size"], hp["patch_size"], hp["num_channels"])
    patches = patchify(image, patch_shape, hp["patch_size"])
    patches = np.reshape(patches, hp["flat_patches_shape"]).astype(np.float32)

    class_name = path.split("/")[-2]
    class_idx = hp["class_names"].index(class_name)
    return patches, np.array(class_idx, dtype=np.int32)

def parse(path):
    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])
    labels = tf.one_hot(labels, hp["num_classes"])
    patches.set_shape(hp["flat_patches_shape"])
    labels.set_shape(hp["num_classes"])
    return patches, labels

def tf_dataset(images, batch=32, augment=False):
    def augment_data(patches, labels):
        patches = tf.image.random_flip_left_right(patches)
        patches = tf.image.random_brightness(patches, max_delta=0.2)
        patches = tf.image.random_contrast(patches, 0.8, 1.2)
        return patches, labels

    ds = tf.data.Dataset.from_tensor_slices((images))
    ds = ds.map(parse, num_parallel_calls=tf.data.AUTOTUNE)
    
    if augment:
        ds = ds.map(augment_data, num_parallel_calls=tf.data.AUTOTUNE)

    ds = ds.shuffle(1000).batch(batch).prefetch(tf.data.AUTOTUNE)
    return ds

if __name__ == "__main__":
    np.random.seed(42)
    tf.random.set_seed(42)

    create_dir("files")

    dataset_path = "/kaggle/input/malnet-dataset/0000"
    model_path = os.path.join("files", "model.keras")
    csv_path = os.path.join("files", "log.csv")

    """ Load Dataset """
    train_x, valid_x, test_x = load_data(dataset_path, val_split=0.2, test_split=0.1)
    train_ds = tf_dataset(train_x, batch=hp["batch_size"], augment=True)
    valid_ds = tf_dataset(valid_x, batch=hp["batch_size"], augment=False)

    """ Initialize Model """
    model = ViT(hp)

    """ Learning Rate Scheduling """
    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
        initial_learning_rate=hp["lr"], decay_steps=1000, alpha=0.1
    )

    """ Compile Model """
    model.compile(
        loss="categorical_crossentropy",
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipvalue=1.0),
        metrics=["accuracy"]
    )

    """ Training Callbacks """
    callbacks = [
        ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),
        CSVLogger(csv_path),
        EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),
    ]

    """ Train Model """
    model.fit(
        train_ds,
        epochs=hp["num_epochs"],
        validation_data=valid_ds,
        callbacks=callbacks
    )
