import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2  # Import L2 Regularization

l2_lambda = 1e-4  # Regularization Strength

class ClassToken(Layer):
    def __init__(self):
        super().__init__()

    def build(self, input_shape):
        w_init = tf.random_normal_initializer()
        self.w = tf.Variable(
            initial_value=w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),
            trainable=True
        )

    def call(self, inputs):
        batch_size = tf.shape(inputs)[0]
        hidden_dim = self.w.shape[-1]

        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])
        cls = tf.cast(cls, dtype=inputs.dtype)
        return cls

def mlp(x, cf):
    """ Multi-Layer Perceptron with L2 Regularization """
    x = Dense(cf["mlp_dim"], activation="gelu", kernel_regularizer=l2(l2_lambda))(x)
    x = Dropout(cf["dropout_rate"])(x)
    x = Dense(cf["hidden_dim"], kernel_regularizer=l2(l2_lambda))(x)
    x = Dropout(cf["dropout_rate"])(x)
    return x

def transformer_encoder(x, cf):
    """ Transformer Encoder with L2 Regularization """
    skip_1 = x
    x = LayerNormalization()(x)
    x = MultiHeadAttention(
        num_heads=cf["num_heads"], key_dim=cf["hidden_dim"],
        kernel_regularizer=l2(l2_lambda)  # Apply L2 Regularization
    )(x, x)
    x = Add()([x, skip_1])

    skip_2 = x
    x = LayerNormalization()(x)
    x = mlp(x, cf)
    x = Add()([x, skip_2])

    return x

def ViT(cf):
    """ Inputs """
    input_shape = (cf["num_patches"], cf["patch_size"]*cf["patch_size"]*cf["num_channels"])
    inputs = Input(input_shape)

    """ Patch + Position Embeddings """
    patch_embed = Dense(cf["hidden_dim"], kernel_regularizer=l2(l2_lambda))(inputs)

    positions = tf.range(start=0, limit=cf["num_patches"], delta=1)
    pos_embed = Embedding(input_dim=cf["num_patches"], output_dim=cf["hidden_dim"])(positions)
    embed = patch_embed + pos_embed

    """ Adding Class Token """
    token = ClassToken()(embed)
    x = Concatenate(axis=1)([token, embed])

    for _ in range(cf["num_layers"]):
        x = transformer_encoder(x, cf)

    """ Classification Head """
    x = LayerNormalization()(x)
    x = x[:, 0, :]
    x = Dense(cf["num_classes"], activation="softmax", kernel_regularizer=l2(l2_lambda))(x)

    model = Model(inputs, x)
    return model

if __name__ == "__main__":
    config = {
        "num_layers": 8,
        "hidden_dim": 512,
        "mlp_dim": 2048,
        "num_heads": 8,
        "dropout_rate": 0.1,
        "num_patches": 64,
        "patch_size": 20,
        "num_channels": 3,
        "num_classes": 5
    }

    model = ViT(config)
    model.summary()
