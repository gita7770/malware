import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
import numpy as np
import tensorflow as tf
import matplotlib
matplotlib.use('Agg')  # Use Agg backend which doesn't require a display
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
from sklearn.metrics import accuracy_score, classification_report
from train import load_data, tf_dataset  # Ensure you have this module available
from vit import ViT  # Ensure you have this module available

""" Hyperparameters """
hp = {}
hp["image_size"] = 200
hp["num_channels"] = 3
hp["patch_size"] = 25
hp["num_patches"] = (hp["image_size"]**2) // (hp["patch_size"]**2)
hp["flat_patches_shape"] = (hp["num_patches"], hp["patch_size"]*hp["patch_size"]*hp["num_channels"])

hp["batch_size"] = 16
hp["lr"] = 1e-4
hp["num_epochs"] = 500
hp["num_classes"] = 5
hp["class_names"] = ["adware", "addisplay", "benign", "trojan", "riskware"]
hp["num_layers"] = 12
hp["hidden_dim"] = 768
hp["mlp_dim"] = 3072
hp["num_heads"] = 12
hp["dropout_rate"] = 0.1

def fgsm_attack(model, images, labels, epsilon, save_path=None):
    images = tf.cast(images, tf.float32)
    with tf.GradientTape() as tape:
        tape.watch(images)
        predictions = model(images)
        loss = tf.keras.losses.categorical_crossentropy(labels, predictions)
    gradient = tape.gradient(loss, images)
    signed_grad = tf.sign(gradient)
    adversarial_images = images + epsilon * signed_grad
    adversarial_images = tf.clip_by_value(adversarial_images, 0, 1)  # Ensure the images are in [0, 1] range
    
    # Save adversarial images if save_path is provided
    if save_path:
        for idx, adv_image in enumerate(adversarial_images):
            plt.imsave(os.path.join('/content/malware/img/gita1', f'fgsm_adv_image_{idx}.png'), adv_image.numpy())
    
    return adversarial_images

def pgd_attack(model, images, labels, epsilon, alpha, iterations, save_path=None):
    images = tf.cast(images, tf.float32)
    original_images = images

    for i in range(iterations):
        with tf.GradientTape() as tape:
            tape.watch(images)
            predictions = model(images)
            loss = tf.keras.losses.categorical_crossentropy(labels, predictions)
        gradient = tape.gradient(loss, images)
        signed_grad = tf.sign(gradient)
        images = images + alpha * signed_grad
        perturbations = tf.clip_by_value(images - original_images, -epsilon, epsilon)
        images = tf.clip_by_value(original_images + perturbations, 0, 1)  # Ensure the images are in [0, 1] range
        
        # Save adversarial images if save_path is provided
        if save_path:
            for idx, adv_image in enumerate(images):
               # plt.imsave(os.path.join(save_path, f'pgd_adv_image_iter{i}_idx{idx}.png'), adv_image.numpy())
               plt.imsave(os.path.join( '/content/malware/img/gita2', f'pgd_adv_image_iter{i}_idx{idx}.png'), adv_image.numpy())
                

    return images

def plot_roc_curve(labels, predictions, title):
    fpr, tpr, _ = roc_curve(labels.ravel(), predictions.ravel())
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{title} (AUC = {roc_auc:.2f})')

if __name__ == "__main__":
    """ Seeding """
    np.random.seed(42)
    tf.random.set_seed(42)

    """ Paths """
    dataset_path = "/content/drive/MyDrive/POP/train"
    model_path = os.path.join("files", "model.h5")
    adversarial_images_save_path = "adversarial_images"  # Directory to save adversarial images

    """ Dataset """
    train_x, valid_x, test_x = load_data(dataset_path)
    print(f"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}")

    test_ds = tf_dataset(test_x, batch=hp["batch_size"])

    """ Model """
    model = ViT(hp)
    model.load_weights(model_path)
    model.compile(
        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
        optimizer=tf.keras.optimizers.Adam(hp["lr"]),
        metrics=["acc"]
    )

    # Evaluate the model on clean data
    print("Evaluating on clean data...")
    clean_loss, clean_acc = model.evaluate(test_ds)
    print(f"Clean data - Loss: {clean_loss}, Accuracy: {clean_acc}")

    # Generate predictions and true labels
    y_true = []
    y_pred = []
    y_pred_proba = []

    for images, labels in test_ds:
        preds = model.predict(images)
        y_true.extend(np.argmax(labels, axis=1))
        y_pred.extend(np.argmax(preds, axis=1))
        y_pred_proba.extend(preds)

    # Calculate the confusion matrix
    cm_clean = confusion_matrix(y_true, y_pred)
    print(cm_clean)

    # Plot the confusion matrix for clean data
    plt.figure(figsize=(8, 6))
    disp_clean = ConfusionMatrixDisplay(confusion_matrix=cm_clean, display_labels=hp["class_names"])
    disp_clean.plot(cmap=plt.cm.viridis)
    plt.title("Confusion Matrix - Clean Data")
    plt.savefig('confusion_matrix_clean.png')
    plt.close()

    # Plot ROC curve for clean data
    plt.figure()
    plot_roc_curve(np.eye(hp["num_classes"])[y_true], np.array(y_pred_proba), "Clean Data")
    plt.title('ROC Curve - Clean Data')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.savefig('roc_curve_clean.png')
    plt.close()

    # Epsilon values to test
    epsilon_values = [0.01, 0.05, 0.1, 0.3, 0.5]

    fgsm_accuracies = []
    pgd_accuracies = []

    alpha = 0.01
    iterations = 10

    for epsilon in epsilon_values:
        # FGSM Attack Evaluation
        print(f"Evaluating under FGSM attack with epsilon={epsilon}...")
        fgsm_acc = tf.metrics.CategoricalAccuracy()
        y_pred_fgsm = []
        y_pred_proba_fgsm = []

        for images, labels in test_ds:
            adv_images = fgsm_attack(model, images, labels, epsilon, adversarial_images_save_path)
            predictions = model(adv_images)
            fgsm_acc.update_state(labels, predictions)
            y_pred_fgsm.extend(np.argmax(predictions, axis=1))
            y_pred_proba_fgsm.extend(predictions)
        
        fgsm_accuracy = fgsm_acc.result().numpy()
        fgsm_accuracies.append(fgsm_accuracy)
        print(f"FGSM attack - Accuracy: {fgsm_accuracy}")

        # Plot ROC curve for FGSM attack
        plt.figure()
        plot_roc_curve(np.eye(hp["num_classes"])[y_true], np.array(y_pred_proba_fgsm), f"FGSM Attack (epsilon={epsilon})")
        plt.title(f'ROC Curve - FGSM Attack (epsilon={epsilon})')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.legend(loc='lower right')
        plt.savefig(f'roc_curve_fgsm_{epsilon}.png')
        plt.close()

        # PGD Attack Evaluation
        print(f"Evaluating under PGD attack with epsilon={epsilon}...")
        pgd_acc = tf.metrics.CategoricalAccuracy()
        y_pred_pgd = []
        y_pred_proba_pgd = []

        for images, labels in test_ds:
            adv_images = pgd_attack(model, images, labels, epsilon, alpha, iterations, adversarial_images_save_path)
            predictions = model(adv_images)
            pgd_acc.update_state(labels, predictions)
            y_pred_pgd.extend(np.argmax(predictions, axis=1))
            y_pred_proba_pgd.extend(predictions)
        
        pgd_accuracy = pgd_acc.result().numpy()
        pgd_accuracies.append(pgd_accuracy)
        print(f"PGD attack - Accuracy: {pgd_accuracy}")

        # Plot ROC curve for PGD attack
        plt.figure()
        plot_roc_curve(np.eye(hp["num_classes"])[y_true], np.array(y_pred_proba_pgd), f"PGD Attack (epsilon={epsilon})")
        plt.title(f'ROC Curve - PGD Attack (epsilon={epsilon})')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.legend(loc='lower right')
        plt.savefig(f'roc_curve_pgd_{epsilon}.png')
        plt.close()

    # Plotting epsilon vs accuracy
    plt.figure(figsize=(10, 6))
    plt.plot(epsilon_values, fgsm_accuracies, marker='o', label='FGSM Attack Accuracy')
    plt.plot(epsilon_values, pgd_accuracies, marker='o', label='PGD Attack Accuracy')
    plt.xlabel('Epsilon')
    plt.ylabel('Accuracy')
    plt.title('Epsilon vs Accuracy')
    plt.legend()
    plt.grid(True)
    plt.savefig('epsilon_vs_accuracy.png')  # Save the figure instead of showing it
    plt.close()
