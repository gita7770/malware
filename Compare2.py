import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
import numpy as np
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score, precision_score, recall_score, f1_score
from train import load_data, tf_dataset  # Ensure you have this module available
from vit import ViT  # Ensure you have this module available

""" Hyperparameters """
hp = {}
hp["image_size"] = 200
hp["num_channels"] = 3
hp["patch_size"] = 25
hp["num_patches"] = (hp["image_size"]**2) // (hp["patch_size"]**2)
hp["flat_patches_shape"] = (hp["num_patches"], hp["patch_size"]*hp["patch_size"]*hp["num_channels"])

hp["batch_size"] = 16
hp["lr"] = 1e-4
hp["num_epochs"] = 500
hp["num_classes"] = 5
hp["class_names"] = ["adware", "addisplay", "benign", "trojan", "riskware"]
hp["num_layers"] = 12
hp["hidden_dim"] = 768
hp["mlp_dim"] = 3072
hp["num_heads"] = 12
hp["dropout_rate"] = 0.1

def fgsm_attack(model, images, labels, epsilon):
    images = tf.cast(images, tf.float32)
    with tf.GradientTape() as tape:
        tape.watch(images)
        predictions = model(images)
        loss = tf.keras.losses.categorical_crossentropy(labels, predictions)
    gradient = tape.gradient(loss, images)
    signed_grad = tf.sign(gradient)
    adversarial_images = images + epsilon * signed_grad
    adversarial_images = tf.clip_by_value(adversarial_images, 0, 1)  # Ensure the images are in [0, 1] range
    return adversarial_images

def pgd_attack(model, images, labels, epsilon, alpha, iterations):
    images = tf.cast(images, tf.float32)
    original_images = images

    for i in range(iterations):
        with tf.GradientTape() as tape:
            tape.watch(images)
            predictions = model(images)
            loss = tf.keras.losses.categorical_crossentropy(labels, predictions)
        gradient = tape.gradient(loss, images)
        signed_grad = tf.sign(gradient)
        images = images + alpha * signed_grad
        perturbations = tf.clip_by_value(images - original_images, -epsilon, epsilon)
        images = tf.clip_by_value(original_images + perturbations, 0, 1)  # Ensure the images are in [0, 1] range

    return images

if __name__ == "__main__":
    """ Seeding """
    np.random.seed(42)
    tf.random.set_seed(42)

    """ Paths """
    dataset_path = "/content/malware/train"
    model_path = os.path.join("files", "/content/drive/MyDrive/model.h5")

    """ Dataset """
    train_x, valid_x, test_x = load_data(dataset_path)
    print(f"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}")

    test_ds = tf_dataset(test_x, batch=hp["batch_size"])

    """ Model """
    model = ViT(hp)
    model.load_weights(model_path)
    model.compile(
        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
        optimizer=tf.keras.optimizers.Adam(hp["lr"]),
        metrics=["acc"]
    )

    # Evaluate the model on clean data
    print("Evaluating on clean data...")
    clean_loss, clean_acc = model.evaluate(test_ds)
    print(f"Clean data - Loss: {clean_loss}, Accuracy: {clean_acc}")

    # Generate predictions and true labels
    y_true = []
    y_pred = []

    for images, labels in test_ds:
        preds = model.predict(images)
        y_true.extend(np.argmax(labels, axis=1))
        y_pred.extend(np.argmax(preds, axis=1))

    # Calculate the confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    print(cm)
     #Plot the confusion matrix using Seaborn
    plt.figure(figsize=(8, 6))  # Adjust the figure size if needed
    sns.heatmap(cm, 
                annot=True,
                fmt='g', 
                cmap='coolwarm',  # Change color map for better visuals
                #xticklabels=['Dog', 'Not Dog'],
                #yticklabels=['Dog', 'Not Dog'],
                cbar=True,  # Add color bar
                linewidths=1,  # Add grid lines
                linecolor='black')  # Color of the grid lines
    plt.xlabel('Prediction', fontsize=13)
    plt.ylabel('Actual', fontsize=13)
    plt.title('Confusion Matrix', fontsize=17)
    plt.tight_layout()  # Adjust layout to make sure everything fits without overlap
    plt.savefig('confusion_matrix.png')  # Save the figure as an image file
    plt.show()
    

    # Plot the confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=hp["class_names"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.show()

    # FGSM Attack Evaluation
    epsilon = 0.5
    print("Evaluating under FGSM attack...")
    fgsm_acc = tf.metrics.CategoricalAccuracy()

    for images, labels in test_ds:
        adv_images = fgsm_attack(model, images, labels, epsilon)
        predictions = model(adv_images)
        fgsm_acc.update_state(labels, predictions)
    
    print(f"FGSM attack - Accuracy: {fgsm_acc.result().numpy()}")

    # PGD Attack Evaluation
    epsilon = 0.01
    alpha = 0.05
    iterations = 40
    print("Evaluating under PGD attack...")
    pgd_acc = tf.metrics.CategoricalAccuracy()

    for images, labels in test_ds:
        adv_images = pgd_attack(model, images, labels, epsilon, alpha, iterations)
        predictions = model(adv_images)
        pgd_acc.update_state(labels, predictions)
    
    print(f"PGD attack - Accuracy: {pgd_acc.result().numpy()}")

    # Evaluate on Clean Image
    y_true_clean = np.argmax(np.concatenate([labels for _, labels in test_ds]), axis=-1)
    y_pred_clean = np.argmax(model.predict(test_ds), axis=-1)

    clean_accuracy = accuracy_score(y_true_clean, y_pred_clean)
    clean_precision = precision_score(y_true_clean, y_pred_clean, average='weighted')
    clean_recall = recall_score(y_true_clean, y_pred_clean, average='weighted')
    clean_f1 = f1_score(y_true_clean, y_pred_clean, average='weighted')

    print(f"Clean data - Accuracy: {clean_accuracy:.4f}, Precision: {clean_precision:.4f}, Recall: {clean_recall:.4f}, F1 Score: {clean_f1:.4f}")

    # Evaluate After FGSM attack
    y_true_fgsm = []
    y_pred_fgsm = []

    for images, labels in test_ds:
        adv_images = fgsm_attack(model, images, labels, epsilon)
        preds = model.predict(adv_images)
        y_true_fgsm.extend(np.argmax(labels, axis=1))
        y_pred_fgsm.extend(np.argmax(preds, axis=1))

    fgsm_accuracy = accuracy_score(y_true_fgsm, y_pred_fgsm)
    fgsm_precision = precision_score(y_true_fgsm, y_pred_fgsm, average='weighted')
    fgsm_recall = recall_score(y_true_fgsm, y_pred_fgsm, average='weighted')
    fgsm_f1 = f1_score(y_true_fgsm, y_pred_fgsm, average='weighted')

    print(f"FGSM attack - Accuracy: {fgsm_accuracy:.4f}, Precision: {fgsm_precision:.4f}, Recall: {fgsm_recall:.4f}, F1 Score: {fgsm_f1:.4f}")

    # Evaluate After PGD Attack
    y_true_pgd = []
    y_pred_pgd = []

    for images, labels in test_ds:
        adv_images = pgd_attack(model, images, labels, epsilon, alpha, iterations)
        preds = model.predict(adv_images)
        y_true_pgd.extend(np.argmax(labels, axis=1))
        y_pred_pgd.extend(np.argmax(preds, axis=1))

    pgd_accuracy = accuracy_score(y_true_pgd, y_pred_pgd)
    pgd_precision = precision_score(y_true_pgd, y_pred_pgd, average='weighted')
    pgd_recall = recall_score(y_true_pgd, y_pred_pgd, average='weighted')
    pgd_f1 = f1_score(y_true_pgd, y_pred_pgd, average='weighted')

    print(f"PGD attack - Accuracy: {pgd_accuracy:.4f}, Precision: {pgd_precision:.4f}, Recall: {pgd_recall:.4f}, F1 Score: {pgd_f1:.4f}")

    # Print classification reports
    print("Classification Report for Clean Data:")
    print(classification_report(y_true_clean, y_pred_clean, target_names=hp["class_names"]))

    print("Classification Report for FGSM Attack:")
    print(classification_report(y_true_fgsm, y_pred_fgsm, target_names=hp["class_names"]))

    print("Classification Report for PGD Attack:")
    print(classification_report(y_true_pgd, y_pred_pgd, target_names=hp["class_names"]))

    # Combined Bar Diagram for Precision, Recall, and F1 Score
    metrics = ['Precision', 'Recall', 'F1 Score']
    clean_scores = [clean_precision, clean_recall, clean_f1]
    fgsm_scores = [fgsm_precision, fgsm_recall, fgsm_f1]
    pgd_scores = [pgd_precision, pgd_recall, pgd_f1]

    x = np.arange(len(metrics))  # the label locations
    width = 0.2  # the width of the bars

    fig, ax = plt.subplots()
    rects1 = ax.bar(x - width, clean_scores, width, label='Clean')
    rects2 = ax.bar(x, fgsm_scores, width, label='FGSM')
    rects3 = ax.bar(x + width, pgd_scores, width, label='PGD')

    # Add some text for labels, title and custom x-axis tick labels, etc.
    ax.set_ylabel('Scores')
    ax.set_title('Precision, Recall, and F1 Score for Clean, FGSM, and PGD')
    ax.set_xticks(x)
    ax.set_xticklabels(metrics)
    ax.legend()

    fig.tight_layout()
    plt.savefig('combined_bar_diagram.png')
    plt.show()
