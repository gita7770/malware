import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import numpy as np
import cv2
from glob import glob
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from patchify import patchify
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping
from vit import ViT  # Ensure you have the ViT implementation

""" Hyperparameters """
hp = {}
hp["image_size"] = 200
hp["num_channels"] = 3
hp["patch_size"] = 25
hp["num_patches"] = (hp["image_size"]**2) // (hp["patch_size"]**2)
hp["flat_patches_shape"] = (hp["num_patches"], hp["patch_size"]*hp["patch_size"]*hp["num_channels"])

hp["batch_size"] = 32
hp["lr"] = 1e-4
hp["num_epochs"] = 500
hp["num_classes"] = 5
hp["class_names"] = ["adware", "addisplay", "benign", "trojan", "riskware"]

hp["num_layers"] = 6  # Reduced complexity
hp["hidden_dim"] = 384  # Reduced hidden dimension
hp["mlp_dim"] = 1536  # Reduced MLP dimension
hp["num_heads"] = 6  # Reduced number of attention heads
hp["dropout_rate"] = 0.2  # Increased dropout rate

def create_dir(path):
    """ Create a directory if it doesn't exist """
    if not os.path.exists(path):
        os.makedirs(path)

def load_data(path, train_split=0.8, val_split=0.2):
    """
    Load and split the dataset into train, validation, and test sets.
    Args:
        path (str): Path to the dataset.
        train_split (float): Fraction of data to use for training (e.g., 0.8 for 80%).
        val_split (float): Fraction of data to use for validation (e.g., 0.2 for 20%).
    Returns:
        train_x (list): List of training image paths.
        valid_x (list): List of validation image paths.
        test_x (list): List of test image paths.
    """
    images = shuffle(glob(os.path.join(path, "*", "*.png")))
    
    # First split: Separate training + validation from test set
    train_val_x, test_x = train_test_split(images, test_size=0.1, random_state=42)  # 10% for testing
    
    # Second split: Separate training and validation sets
    train_x, valid_x = train_test_split(train_val_x, test_size=val_split, random_state=42)
    
    return train_x, valid_x, test_x

def augment_image(image):
    """ Apply data augmentation to an image """
    # Random horizontal flip
    if tf.random.uniform(()) > 0.5:
        image = tf.image.flip_left_right(image)
    
    # Random rotation
    image = tf.image.rot90(image, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))
    
    # Random brightness and contrast adjustment
    image = tf.image.random_brightness(image, max_delta=0.2)
    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)
    
    # Random zoom and crop
    image = tf.image.central_crop(image, central_fraction=tf.random.uniform(shape=[], minval=0.8, maxval=1.0))
    image = tf.image.resize(image, [hp["image_size"], hp["image_size"]])
    
    return image

def process_image_label(path, augment=False):
    """ Process an image into patches and extract its label """
    path = path.decode()
    image = cv2.imread(path, cv2.IMREAD_COLOR)
    image = cv2.resize(image, (hp["image_size"], hp["image_size"]))
    image = image / 255.0  # Normalize to [0, 1]

    # Apply data augmentation if enabled
    if augment:
        image = augment_image(image)

    # Split image into patches
    patch_shape = (hp["patch_size"], hp["patch_size"], hp["num_channels"])
    patches = patchify(image, patch_shape, hp["patch_size"])
    patches = np.reshape(patches, hp["flat_patches_shape"])
    patches = patches.astype(np.float32)

    # Extract label
    class_name = path.split("/")[-2]
    class_idx = hp["class_names"].index(class_name)
    class_idx = np.array(class_idx, dtype=np.int32)

    return patches, class_idx

def parse(path, augment=False):
    """ Parse the image and label for TensorFlow dataset """
    patches, labels = tf.numpy_function(process_image_label, [path, augment], [tf.float32, tf.int32])
    labels = tf.one_hot(labels, hp["num_classes"])

    patches.set_shape(hp["flat_patches_shape"])
    labels.set_shape(hp["num_classes"])

    return patches, labels

def tf_dataset(images, batch=32, augment=False):
    """ Create a TensorFlow dataset from images """
    ds = tf.data.Dataset.from_tensor_slices((images))
    ds = ds.map(lambda x: parse(x, augment), num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)
    return ds

if __name__ == "__main__":
    """ Seeding """
    np.random.seed(42)
    tf.random.set_seed(42)

    """ Directory for storing files """
    create_dir("files")

    """ Paths """
    dataset_path = "/content/drive/MyDrive/Malnet"  # Update this path
    model_path = os.path.join("files", "model.h5")
    csv_path = os.path.join("files", "log.csv")

     """ Dataset """
    train_x, valid_x, test_x = load_data(dataset_path, train_split=0.8, val_split=0.2)
    print(f"Train: {len(train_x)} - Valid: {len(valid_x)} - Test: {len(test_x)}")

    # Enable augmentation for the training dataset
    train_ds = tf_dataset(train_x, batch=hp["batch_size"], augment=True)
    valid_ds = tf_dataset(valid_x, batch=hp["batch_size"])

    """ Model """
    model = ViT(hp)
    model.compile(
        loss="categorical_crossentropy",
        optimizer=tf.keras.optimizers.Adam(hp["lr"], clipvalue=1.0),
        metrics=["acc"]
    )

    """ Callbacks """
    callbacks = [
        ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),
        CSVLogger(csv_path),
        EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),
    ]

    """ Training """
    history = model.fit(
        train_ds,
        epochs=hp["num_epochs"],
        validation_data=valid_ds,
        callbacks=callbacks
    )

    """ Evaluation """
    test_ds = tf_dataset(test_x, batch=hp["batch_size"])
    test_loss, test_acc = model.evaluate(test_ds)
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {test_acc:.4f}")
